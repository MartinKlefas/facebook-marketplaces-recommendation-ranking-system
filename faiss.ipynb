{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a FAISS Search based on https://towardsdatascience.com/understanding-faiss-619bb6db2d1a & https://www.pinecone.io/learn/faiss-tutorial/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[0.41342562 0.39609057 0.29425067 ... 0.10239422 0.22096485 0.5322781 ]\n",
      "2048 12604\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import faiss , sys\n",
    "\n",
    "newImageList = pd.read_json(\"embeddings.json\")\n",
    "#change the individual rows into numpy arrays of type float32\n",
    "db_vectors = [np.array(x, dtype=\"float32\") for x in newImageList['embedding']]\n",
    "\n",
    "#change it from a list of arrays to an array of arrays.\n",
    "db_vectors = np.array(db_vectors)\n",
    "\n",
    "#check everything went ok\n",
    "print(type(db_vectors))\n",
    "print( type(db_vectors[0]))\n",
    "print(db_vectors[0])\n",
    "\n",
    "dimension = len(db_vectors[0])    # dimensions of each vector                         \n",
    "n = len(db_vectors)    # number of vectors  \n",
    "\n",
    "print(dimension,n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we've got an array of arrays in memory. Cool, let's index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = int(13)  # number of clusters\n",
    "quantiser = faiss.IndexFlatL2(dimension)  \n",
    "index = faiss.IndexIVFFlat(quantiser, dimension, nlist,   faiss.METRIC_L2)\n",
    "\n",
    "index.train(db_vectors)\n",
    "index.add(db_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that seems to have worked. Let's save the index to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index,\"images_faiss.index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try an example search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import image_processor\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torch.load('model_evaluation/feature/best_weights.pt')\n",
    "searchEmbedding  = image_processor.getEmbedding(image='images/clean_image_data/0a1baaa8-4556-4e07-a486-599c05cce76c.jpg',model=model)\n",
    "searchEmbedding = np.array(searchEmbedding,dtype=\"float32\",ndmin=2)\n",
    "\n",
    "nprobe = 5\n",
    "distances, indices = index.search(x=searchEmbedding,k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6185     True\n",
      "5682    False\n",
      "5680    False\n",
      "Name: id, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(newImageList[\"id\"][indices[0]] == \"0a1baaa8-4556-4e07-a486-599c05cce76c\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Facebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
